{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid, cross_validate\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import decomposition\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import itertools\n",
    "import random\n",
    "import warnings\n",
    "import string\n",
    "from shutil import copyfile\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "from functions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_shuffle_seed = 4\n",
    "global_debug=True\n",
    "global_override=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 1380 True: 663 False: 717\n"
     ]
    }
   ],
   "source": [
    "df_data = pd.read_json(\"/Users/jiaying/Desktop/6506 DSPP/Data Assignment #2/data/classifier/data/data.json\")\n",
    "df_data = df_data[df_data['is_flood'].notna()]\n",
    "data_true = query_dataframe(df_data, {'is_flood':True})\n",
    "data_false = query_dataframe(df_data, {'is_flood':False})\n",
    "print('Total:',len(df_data),'True:',len(data_true), 'False:',len(data_false))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = set(['date', 'published'])\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "punctuations = set(string.punctuation)\n",
    "all_stop_words = stop_words.union(punctuations, custom_stop_words)\n",
    "def preprocess(x):\n",
    "    x = re.sub('[^a-z\\s]', ' ', x.lower())\n",
    "    x = [w for w in x.split() if w not in all_stop_words and len(w)>3]\n",
    "    return ' '.join(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['org_text'] = df_data['text']\n",
    "df_data['text'] = df_data['text'].apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>is_flood</th>\n",
       "      <th>is_bangladesh</th>\n",
       "      <th>flood_related</th>\n",
       "      <th>flood_climatechange</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>flood_type</th>\n",
       "      <th>dates</th>\n",
       "      <th>...</th>\n",
       "      <th>event_damage-damage_info_other</th>\n",
       "      <th>event_damage-people_affected</th>\n",
       "      <th>event_damage-peopled_displaced</th>\n",
       "      <th>event_damage-homes_affected</th>\n",
       "      <th>event_damage-disease</th>\n",
       "      <th>event_damage-fatalities</th>\n",
       "      <th>event_dates</th>\n",
       "      <th>event_dates-date</th>\n",
       "      <th>event_dates-prev_date</th>\n",
       "      <th>org_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ec583817-3c60-41ee-b856-65f0d9bd7772</td>\n",
       "      <td>dailySun_data_ec583817-3c60-41ee-b856-65f0d9bd...</td>\n",
       "      <td>tuesday july rise water level erosion madhumat...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Date Published:2017-08-31 06:03:11+00:00 tuesd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f4806621-b874-4f20-97fb-f7c1fa94f6bc</td>\n",
       "      <td>theDailyStar_data_f4806621-b874-4f20-97fb-f7c1...</td>\n",
       "      <td>flash floods triggered heavy rain hailstorms d...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>daily_star</td>\n",
       "      <td>flash</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Date Published:2016-05-05 00:00:00 Flash flood...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>259d503d-f6b1-44b6-a866-8eff03799a07</td>\n",
       "      <td>prothomalo_data_259d503d-f6b1-44b6-a866-8eff03...</td>\n",
       "      <td>none flood situation worsened tuesday increasi...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Date Published:None Flood situation worsened f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>1989_34feabf9fb9bd7378a2a378bbc72e5c80ce50f43.txt</td>\n",
       "      <td>world learns geography quiz globally educated ...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ny_times</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>AS THE WORLD LEARNS; Geography Quiz\\n1989-04-0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f4182e23-bcd2-4627-b5ee-6284ac4a2eac</td>\n",
       "      <td>dhakaTribune_data_f4182e23-bcd2-4627-b5ee-6284...</td>\n",
       "      <td>deceased shahadat tareque additional three peo...</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>dhaka_tribune</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Date Published:2019-06-14 00:00:00      \\nThe ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 doc_id  \\\n",
       "0  ec583817-3c60-41ee-b856-65f0d9bd7772   \n",
       "1  f4806621-b874-4f20-97fb-f7c1fa94f6bc   \n",
       "2  259d503d-f6b1-44b6-a866-8eff03799a07   \n",
       "3                                  None   \n",
       "4  f4182e23-bcd2-4627-b5ee-6284ac4a2eac   \n",
       "\n",
       "                                            filename  \\\n",
       "0  dailySun_data_ec583817-3c60-41ee-b856-65f0d9bd...   \n",
       "1  theDailyStar_data_f4806621-b874-4f20-97fb-f7c1...   \n",
       "2  prothomalo_data_259d503d-f6b1-44b6-a866-8eff03...   \n",
       "3  1989_34feabf9fb9bd7378a2a378bbc72e5c80ce50f43.txt   \n",
       "4  dhakaTribune_data_f4182e23-bcd2-4627-b5ee-6284...   \n",
       "\n",
       "                                                text  is_flood  is_bangladesh  \\\n",
       "0  tuesday july rise water level erosion madhumat...      True            NaN   \n",
       "1  flash floods triggered heavy rain hailstorms d...      True            1.0   \n",
       "2  none flood situation worsened tuesday increasi...      True            NaN   \n",
       "3  world learns geography quiz globally educated ...     False            0.0   \n",
       "4  deceased shahadat tareque additional three peo...     False            1.0   \n",
       "\n",
       "   flood_related  flood_climatechange      newspaper flood_type dates  ...  \\\n",
       "0            NaN                  NaN           None       None    []  ...   \n",
       "1            1.0                  0.0     daily_star      flash    []  ...   \n",
       "2            NaN                  NaN           None       None    []  ...   \n",
       "3            0.0                  0.0       ny_times       None    []  ...   \n",
       "4            0.0                  0.0  dhaka_tribune       None    []  ...   \n",
       "\n",
       "  event_damage-damage_info_other event_damage-people_affected  \\\n",
       "0                             []                           []   \n",
       "1                             []                           []   \n",
       "2                             []                           []   \n",
       "3                             []                           []   \n",
       "4                             []                           []   \n",
       "\n",
       "  event_damage-peopled_displaced event_damage-homes_affected  \\\n",
       "0                             []                          []   \n",
       "1                             []                          []   \n",
       "2                             []                          []   \n",
       "3                             []                          []   \n",
       "4                             []                          []   \n",
       "\n",
       "  event_damage-disease event_damage-fatalities event_dates event_dates-date  \\\n",
       "0                   []                      []          []               []   \n",
       "1                   []                      []          []               []   \n",
       "2                   []                      []          []               []   \n",
       "3                   []                      []          []               []   \n",
       "4                   []                      []          []               []   \n",
       "\n",
       "  event_dates-prev_date                                           org_text  \n",
       "0                    []  Date Published:2017-08-31 06:03:11+00:00 tuesd...  \n",
       "1                    []  Date Published:2016-05-05 00:00:00 Flash flood...  \n",
       "2                    []  Date Published:None Flood situation worsened f...  \n",
       "3                    []  AS THE WORLD LEARNS; Geography Quiz\\n1989-04-0...  \n",
       "4                    []  Date Published:2019-06-14 00:00:00      \\nThe ...  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_ratio(df_data, test_size=None, train_size=None, shuffle_seed=4, debug=False, \n",
    "                    save_folder=None, load_folder=None, override=False, file_prefix=''):\n",
    "    save_file, load_file=None, None\n",
    "    if save_folder: save_file = os.path.join(save_folder,file_prefix+'data.json')\n",
    "    if load_folder: load_file = os.path.join(load_folder,file_prefix+'data.json')\n",
    "    \n",
    "    if not override and load_file and os.path.isfile(load_file):\n",
    "        if debug: print('loaded',load_file)\n",
    "        js = json.load(open(load_file))\n",
    "        train_df = pd.DataFrame(js['train'])\n",
    "        test_df = pd.DataFrame(js['test'])\n",
    "        return {'train':train_df, 'test':test_df}\n",
    "    \n",
    "    train_df, test_df = train_test_split(df_data, test_size=test_size, train_size=train_size, random_state=shuffle_seed, stratify=df_data['is_flood'])\n",
    "    \n",
    "    if debug: print('Data Loaded')\n",
    "\n",
    "    if save_file:\n",
    "        train_json = train_df.to_json(orient='records')\n",
    "        test_json = test_df.to_json(orient='records')\n",
    "        json.dump({'train':json.loads(train_json), 'test':json.loads(test_json)}, open(save_file,'w'), indent=2)\n",
    "    return {'train':train_df, 'test':test_df}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded\n"
     ]
    }
   ],
   "source": [
    "save_data_folder = \"/Users/jiaying/Desktop/6506 DSPP/Data Assignment #2/code/classifier\"\n",
    "\n",
    "result, clf_result = {}, {}\n",
    "test_size = 0.2\n",
    "\n",
    "if not os.path.isdir(save_data_folder): os.mkdir(save_data_folder)\n",
    "debug=global_debug or False\n",
    "override=global_override or False\n",
    "data_split = make_data_ratio(df_data, test_size=test_size,\n",
    "                               debug=debug, shuffle_seed=global_shuffle_seed, override=override)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1104 \t\tTest: 276\n",
      "Train is_flood: 530 \tTrain not is_flood: 574\n",
      "Test is_flood: 133 \tTest not is_flood: 143\n"
     ]
    }
   ],
   "source": [
    "print('Train:',len(data_split['train']), '\\t\\tTest:',len(data_split['test']))\n",
    "print('Train is_flood:',len(data_split['train'].loc[data_split['train']['is_flood']==True]), \\\n",
    "'\\tTrain not is_flood:',len(data_split['train'].loc[data_split['train']['is_flood']==False]))\n",
    "print('Test is_flood:',len(data_split['test'].loc[data_split['test']['is_flood']==True]), \\\n",
    "'\\tTest not is_flood:',len(data_split['test'].loc[data_split['test']['is_flood']==False]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: Regular run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(vect_fit, ratio):\n",
    "    train, test = ratio.get('train',None), ratio.get('test',None)\n",
    "    if train is None or test is None: raise Exception('Train or Test data not found')\n",
    "    all_X = list(train['text'])\n",
    "    \n",
    "    vect = vect_fit.fit(all_X)\n",
    "    trainX, testX = vect.transform(list(train['text'])), vect.transform(list(test['text']))\n",
    "    trainY, testY = [1 if t else 0 for t in train['is_flood']], [1 if t else 0 for t in test['is_flood']]\n",
    "    return trainX, testX, trainY, testY, vect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classifier(clf, trainX, testX, trainY, testY):\n",
    "    clf_fit = clf.fit(trainX, trainY)\n",
    "    clf_pred = clf_fit.predict(testX)\n",
    "    clf_acc = accuracy_score(testY, clf_pred)\n",
    "    return clf_fit, clf_pred, clf_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_method(main_d, name):\n",
    "    if name not in main_d: raise Exception('Cannot find classifier/feature_extractor name in parameter dictionary')\n",
    "    d = main_d[name]\n",
    "    method = d.get('method',None)\n",
    "    base_method = d.get('base_method',None)\n",
    "    if method and base_method: raise Exception('Cannot have method and base method both.')\n",
    "    if not method and not base_method: raise Exception('Unable to parse the method from classifier/feature_extractor')\n",
    "    params = d.get('params',None)\n",
    "    if method:\n",
    "        if params: return method, params\n",
    "        else: return method, None\n",
    "    if base_method:\n",
    "        prev_method, prev_params = get_method(main_d, base_method)\n",
    "        if params:\n",
    "            for k,v in params.items(): prev_params[k] = v\n",
    "        return prev_method, prev_params\n",
    "\n",
    "def make_method(main_d, name, override_params={}):\n",
    "    method, params = get_method(main_d, name)[:]\n",
    "    if override_params:\n",
    "        for k,v in override_params.items(): params[k] = v\n",
    "    if params: return method(**params)\n",
    "    else: return method()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grid(grid, data, feature_extract, classifiers, clf_result, result, \n",
    "             debug=False, override=False, save_folder=None, load_folder=None, file_prefix=''):\n",
    "    save_clf_result = {}\n",
    "    vectCache, classifierCache = {}, {}\n",
    "    if load_folder:\n",
    "        res_file = os.path.join(load_folder,file_prefix+'clf_result.json')\n",
    "        clf_res_file = os.path.join(load_folder,file_prefix+'result.json')\n",
    "        if os.path.isfile(res_file): clf_result=json.load(open(res_file))\n",
    "        if os.path.isfile(clf_res_file): result=json.load(open(clf_res_file))\n",
    "        if os.path.isfile(res_file) and os.path.isfile(clf_res_file) and debug: print('loaded result')\n",
    "    \n",
    "    if override:\n",
    "        clf_result, result = {}, {}\n",
    "        if debug: print('OVERRIDE')\n",
    "    for g in list(grid):\n",
    "        try:\n",
    "            feature_name = g.get('feature_extract',None)\n",
    "            clf_name = g.get('classifier', None)\n",
    "            if not feature_name or not clf_name:\n",
    "                raise Exception('Feature Extract and Classifier Name required')\n",
    "            result_key = feature_name + '-' + clf_name\n",
    "            if result.get(result_key): continue\n",
    "            if debug: print('Feature:', feature_name, '  Clasifier:',clf_name, '  Key:',result_key)\n",
    "            \n",
    "            if feature_name in vectCache:\n",
    "                (trainX, testX, trainY, testY, feature2) = vectCache[feature_name]\n",
    "            else:\n",
    "                feature = make_method(feature_extract, feature_name)\n",
    "                trainX, testX, trainY, testY, feature2 = make_data(feature, data)\n",
    "                vectCache[feature_name] = (trainX, testX, trainY, testY, feature2)\n",
    "\n",
    "            clf = make_method(classifiers, clf_name)\n",
    "            clf_fit, clf_pred, clf_acc = run_classifier(clf, trainX, testX, trainY, testY)\n",
    "            \n",
    "            result[result_key] = {\n",
    "                'feature_extract': feature_name,\n",
    "                'classifier': clf_name,\n",
    "                'accuracy': clf_acc\n",
    "            }\n",
    "            \n",
    "            clf_result[result_key] = {\n",
    "                'feature_extract': feature_name,\n",
    "                'classifier': clf_name,\n",
    "                'clf': clf_fit,\n",
    "                'feature': feature2,\n",
    "                'predict': clf_pred\n",
    "            }\n",
    "            \n",
    "            save_clf_result[result_key] = {\n",
    "                'feature_extract': feature_name,\n",
    "                'classifier': clf_name,\n",
    "                'predict': clf_pred.tolist()\n",
    "            }  \n",
    "        except Exception as e:\n",
    "            print('Error:',e)\n",
    "            continue\n",
    "    if save_folder:\n",
    "        json.dump(save_clf_result, open(os.path.join(load_folder,file_prefix+'clf_result.json'),'w'), indent=2)\n",
    "        json.dump(result, open(os.path.join(load_folder,file_prefix+'result.json'),'w'), indent=2)\n",
    "    return clf_result, result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grid_cross_validate(grid, data, feature_extract, classifiers, result, debug=False):\n",
    "    for g in list(grid):\n",
    "        feature_name = g.get('feature_extract',None)\n",
    "        clf_name = g.get('classifier', None)\n",
    "        if not feature_name or not clf_name:\n",
    "            raise Exception('Feature Extract and Classifier Name required')\n",
    "        result_key = feature_name + '-' + clf_name\n",
    "        if result.get(result_key): continue\n",
    "        if debug: print('Feature:', feature_name, '  Clasifier:',clf_name, '  Key:',result_key)\n",
    "\n",
    "        feature = make_method(feature_extract, 'TFIDF')\n",
    "        all_X = data['text']\n",
    "        all_Y = data['is_flood']\n",
    "        vect = feature.fit(all_X)\n",
    "        x, y = vect.transform(list(all_X)), [1 if t else 0 for t in all_Y]\n",
    "        clf = make_method(classifiers, clf_name)\n",
    "        cv = cross_validate(clf, x, y, cv=5,\n",
    "                      scoring=('accuracy', 'precision', 'recall', 'f1'))\n",
    "        result[result_key] = cv\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_result(result, clf_result=None, data=None, accuracy_threshold=None, split_by='classifier'):\n",
    "    keys = list(result.keys())\n",
    "    temp_df = pd.DataFrame(list(result.values()))\n",
    "    if clf_result is not None and data is not None:\n",
    "        presicion, recall, f1, support = [], [], [], []\n",
    "        actual = [1 if f else 0 for f in list(data['test']['is_flood'])]\n",
    "        for method_name in keys:\n",
    "            predict = clf_result[method_name]['predict']\n",
    "            pre, rec, fsc, sup = precision_recall_fscore_support(actual, predict, average='binary')\n",
    "            presicion.append(pre)\n",
    "            recall.append(rec)\n",
    "            f1.append(fsc)\n",
    "            support.append(sup)\n",
    "        temp_df['f1'] = f1\n",
    "        temp_df['presicion'] = presicion\n",
    "        temp_df['recall'] = recall\n",
    "#     temp_df['keys'] = keys\n",
    "    splt_val = list(set(list(temp_df[split_by])))\n",
    "    for d in splt_val:\n",
    "        if accuracy_threshold:\n",
    "            new_df = temp_df.loc[temp_df[split_by]==d]\n",
    "            new_df = new_df.loc[new_df['accuracy']>accuracy_threshold] \\\n",
    "                            .drop(split_by, axis=1) \\\n",
    "                            .sort_values(by='accuracy',ascending=False) \\\n",
    "                            .reset_index(drop=True)\n",
    "        else:\n",
    "            new_df = temp_df.loc[temp_df[split_by]==d] \\\n",
    "                            .drop(split_by, axis=1) \\\n",
    "                            .sort_values(by='accuracy',ascending=False) \\\n",
    "                            .reset_index(drop=True)\n",
    "        print('{}: {}'.format(split_by,d))\n",
    "        print(new_df.to_markdown())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_result(clf_result, data, method_name, conf_matrix=True, class_report=True):\n",
    "    if method_name not in clf_result: raise Exception('Cannot find method')\n",
    "    res = clf_result[method_name]\n",
    "    new_df = data['test']\n",
    "    actual = [1 if f else 0 for f in list(new_df['is_flood'])]\n",
    "    new_df['predict'] = res['predict']\n",
    "    if conf_matrix:\n",
    "        mat = confusion_matrix(actual, res['predict'])\n",
    "        plot_confusion_matrix(mat, ['Negative', 'Positive'])\n",
    "        print(mat)\n",
    "    if class_report: print(classification_report(actual, res['predict']))\n",
    "    cr = classification_report(actual, res['predict'])\n",
    "    return new_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "    accuracy = np.trace(cm) / np.sum(cm).astype('float')\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(ratio):\n",
    "    train, test = ratio.get('train',None), ratio.get('test',None)\n",
    "    if train is None or test is None: raise Exception('Train or Test data not found')\n",
    "    all_X = list(train['text']) + list(test['text'])\n",
    "    \n",
    "    params= {\n",
    "            'tokenizer': word_tokenize,\n",
    "            'stop_words': 'english',\n",
    "        }\n",
    "    vect = CountVectorizer(**params)\n",
    "    vect = vect.fit(all_X)\n",
    "    return list(vect.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "vocab = make_vocab(data_split)\n",
    "feature_extract = {\n",
    "    'CountVect': {\n",
    "        'classifier_type': 'Count Vectorizer',\n",
    "        'method': CountVectorizer,\n",
    "        'params': {\n",
    "            'tokenizer': word_tokenize,\n",
    "            'stop_words': 'english',\n",
    "            'vocabulary': vocab\n",
    "        }\n",
    "    },\n",
    "    'CountVect-2gram':{\n",
    "        'base_method': 'CountVect',\n",
    "        'params':{\n",
    "            'ngram_range':(1,2)\n",
    "        }\n",
    "    },\n",
    "    'CountVect-min_df-max_df':{\n",
    "        'base_method': 'CountVect',\n",
    "        'params':{\n",
    "            'min_df': 0.05,\n",
    "            'max_df': 0.95\n",
    "        }\n",
    "    },\n",
    "    'CountVect-2gram-min_df-max_df':{\n",
    "        'base_method': 'CountVect',\n",
    "        'params':{\n",
    "            'min_df': 0.05,\n",
    "            'max_df': 0.95,\n",
    "            'ngram_range':(1,2)\n",
    "        }\n",
    "    },\n",
    "    'TFIDF': {\n",
    "        'classifier_type': 'TFIDF',\n",
    "        'method': TfidfVectorizer,\n",
    "        'params': {\n",
    "            'tokenizer': word_tokenize,\n",
    "            'stop_words': 'english',\n",
    "            'vocabulary': vocab\n",
    "        }\n",
    "    },\n",
    "    'TFIDF-2gram':{\n",
    "        'base_method': 'TFIDF',\n",
    "        'params':{\n",
    "            'ngram_range':(1,2)\n",
    "        }\n",
    "    },\n",
    "    'TFIDF-min_df-max_df':{\n",
    "        'base_method': 'TFIDF',\n",
    "        'params':{\n",
    "            'min_df': 0.05,\n",
    "            'max_df': 0.95\n",
    "        }\n",
    "    },\n",
    "    'TFIDF-2gram-min_df-max_df':{\n",
    "        'base_method': 'TFIDF',\n",
    "        'params':{\n",
    "            'min_df': 0.05,\n",
    "            'max_df': 0.95,\n",
    "            'ngram_range':(1,2)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "classifiers = {\n",
    "    'RandomForest': {\n",
    "        'classifier_type':'Random Forest ',\n",
    "        'method': RandomForestClassifier,\n",
    "        'params':{\n",
    "            'class_weight':'balanced'\n",
    "        }\n",
    "    },\n",
    "    'LinearSVC': {\n",
    "        'classifier_type': 'Linear SVC',\n",
    "        'method': LinearSVC,\n",
    "        'params':{\n",
    "            'class_weight':'balanced'\n",
    "        }\n",
    "    },\n",
    "    'LogRegL1':{\n",
    "        'classifier_type': 'Logistic Regression L1',\n",
    "        'method': LogisticRegression,\n",
    "        'params':{\n",
    "            'penalty': 'l1',\n",
    "            'class_weight':'balanced',\n",
    "            'solver': 'liblinear',\n",
    "            'max_iter': 1000\n",
    "        }\n",
    "    },\n",
    "    'LogRegL2':{\n",
    "        'classifier_type': 'Logistic Regression L2',\n",
    "        'method': LogisticRegression,\n",
    "        'params':{\n",
    "            'penalty': 'l2',\n",
    "            'class_weight':'balanced',\n",
    "            'solver': 'liblinear',\n",
    "            'max_iter': 1000\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "grid_parameters = {\n",
    "    'feature_extract': list(feature_extract.keys()),\n",
    "    'classifier': list(classifiers.keys()),\n",
    "}\n",
    "\n",
    "grid = ParameterGrid(grid_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded result\n",
      "OVERRIDE\n",
      "Feature: CountVect   Clasifier: RandomForest   Key: CountVect-RandomForest\n",
      "Feature: CountVect-2gram   Clasifier: RandomForest   Key: CountVect-2gram-RandomForest\n",
      "Feature: CountVect-min_df-max_df   Clasifier: RandomForest   Key: CountVect-min_df-max_df-RandomForest\n",
      "Feature: CountVect-2gram-min_df-max_df   Clasifier: RandomForest   Key: CountVect-2gram-min_df-max_df-RandomForest\n",
      "Feature: TFIDF   Clasifier: RandomForest   Key: TFIDF-RandomForest\n",
      "Feature: TFIDF-2gram   Clasifier: RandomForest   Key: TFIDF-2gram-RandomForest\n",
      "Feature: TFIDF-min_df-max_df   Clasifier: RandomForest   Key: TFIDF-min_df-max_df-RandomForest\n",
      "Feature: TFIDF-2gram-min_df-max_df   Clasifier: RandomForest   Key: TFIDF-2gram-min_df-max_df-RandomForest\n",
      "Feature: CountVect   Clasifier: LinearSVC   Key: CountVect-LinearSVC\n",
      "Feature: CountVect-2gram   Clasifier: LinearSVC   Key: CountVect-2gram-LinearSVC\n",
      "Feature: CountVect-min_df-max_df   Clasifier: LinearSVC   Key: CountVect-min_df-max_df-LinearSVC\n",
      "Feature: CountVect-2gram-min_df-max_df   Clasifier: LinearSVC   Key: CountVect-2gram-min_df-max_df-LinearSVC\n",
      "Feature: TFIDF   Clasifier: LinearSVC   Key: TFIDF-LinearSVC\n",
      "Feature: TFIDF-2gram   Clasifier: LinearSVC   Key: TFIDF-2gram-LinearSVC\n",
      "Feature: TFIDF-min_df-max_df   Clasifier: LinearSVC   Key: TFIDF-min_df-max_df-LinearSVC\n",
      "Feature: TFIDF-2gram-min_df-max_df   Clasifier: LinearSVC   Key: TFIDF-2gram-min_df-max_df-LinearSVC\n",
      "Feature: CountVect   Clasifier: LogRegL1   Key: CountVect-LogRegL1\n",
      "Feature: CountVect-2gram   Clasifier: LogRegL1   Key: CountVect-2gram-LogRegL1\n",
      "Feature: CountVect-min_df-max_df   Clasifier: LogRegL1   Key: CountVect-min_df-max_df-LogRegL1\n",
      "Feature: CountVect-2gram-min_df-max_df   Clasifier: LogRegL1   Key: CountVect-2gram-min_df-max_df-LogRegL1\n",
      "Feature: TFIDF   Clasifier: LogRegL1   Key: TFIDF-LogRegL1\n",
      "Feature: TFIDF-2gram   Clasifier: LogRegL1   Key: TFIDF-2gram-LogRegL1\n",
      "Feature: TFIDF-min_df-max_df   Clasifier: LogRegL1   Key: TFIDF-min_df-max_df-LogRegL1\n",
      "Feature: TFIDF-2gram-min_df-max_df   Clasifier: LogRegL1   Key: TFIDF-2gram-min_df-max_df-LogRegL1\n",
      "Feature: CountVect   Clasifier: LogRegL2   Key: CountVect-LogRegL2\n",
      "Feature: CountVect-2gram   Clasifier: LogRegL2   Key: CountVect-2gram-LogRegL2\n",
      "Feature: CountVect-min_df-max_df   Clasifier: LogRegL2   Key: CountVect-min_df-max_df-LogRegL2\n",
      "Feature: CountVect-2gram-min_df-max_df   Clasifier: LogRegL2   Key: CountVect-2gram-min_df-max_df-LogRegL2\n",
      "Feature: TFIDF   Clasifier: LogRegL2   Key: TFIDF-LogRegL2\n",
      "Feature: TFIDF-2gram   Clasifier: LogRegL2   Key: TFIDF-2gram-LogRegL2\n",
      "Feature: TFIDF-min_df-max_df   Clasifier: LogRegL2   Key: TFIDF-min_df-max_df-LogRegL2\n",
      "Feature: TFIDF-2gram-min_df-max_df   Clasifier: LogRegL2   Key: TFIDF-2gram-min_df-max_df-LogRegL2\n"
     ]
    }
   ],
   "source": [
    "override=global_override or False\n",
    "debug=global_debug or False\n",
    "save_results_folder = 'results/'\n",
    "load_results_folder = 'results/'\n",
    "if not os.path.isdir(save_results_folder): os.mkdir(save_results_folder)\n",
    "clf_result, result = run_grid(grid, data_split, feature_extract, classifiers, clf_result, result, \n",
    "                              debug=debug, override=override, save_folder=save_results_folder, \n",
    "                             load_folder=load_results_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier: LinearSVC\n",
      "|    | feature_extract               |   accuracy |       f1 |   presicion |   recall |\n",
      "|---:|:------------------------------|-----------:|---------:|------------:|---------:|\n",
      "|  0 | TFIDF                         |   0.913043 | 0.911111 |    0.89781  | 0.924812 |\n",
      "|  1 | TFIDF-2gram                   |   0.913043 | 0.911111 |    0.89781  | 0.924812 |\n",
      "|  2 | TFIDF-min_df-max_df           |   0.913043 | 0.911111 |    0.89781  | 0.924812 |\n",
      "|  3 | TFIDF-2gram-min_df-max_df     |   0.913043 | 0.911111 |    0.89781  | 0.924812 |\n",
      "|  4 | CountVect                     |   0.905797 | 0.902256 |    0.902256 | 0.902256 |\n",
      "|  5 | CountVect-2gram               |   0.905797 | 0.900763 |    0.914729 | 0.887218 |\n",
      "|  6 | CountVect-min_df-max_df       |   0.902174 | 0.897338 |    0.907692 | 0.887218 |\n",
      "|  7 | CountVect-2gram-min_df-max_df |   0.902174 | 0.897338 |    0.907692 | 0.887218 |\n",
      "\n",
      "classifier: LogRegL2\n",
      "|    | feature_extract               |   accuracy |       f1 |   presicion |   recall |\n",
      "|---:|:------------------------------|-----------:|---------:|------------:|---------:|\n",
      "|  0 | TFIDF                         |   0.913043 | 0.911111 |    0.89781  | 0.924812 |\n",
      "|  1 | TFIDF-2gram                   |   0.913043 | 0.911111 |    0.89781  | 0.924812 |\n",
      "|  2 | TFIDF-min_df-max_df           |   0.913043 | 0.911111 |    0.89781  | 0.924812 |\n",
      "|  3 | TFIDF-2gram-min_df-max_df     |   0.913043 | 0.911111 |    0.89781  | 0.924812 |\n",
      "|  4 | CountVect                     |   0.905797 | 0.903704 |    0.890511 | 0.917293 |\n",
      "|  5 | CountVect-2gram               |   0.905797 | 0.903704 |    0.890511 | 0.917293 |\n",
      "|  6 | CountVect-min_df-max_df       |   0.905797 | 0.903704 |    0.890511 | 0.917293 |\n",
      "|  7 | CountVect-2gram-min_df-max_df |   0.905797 | 0.903704 |    0.890511 | 0.917293 |\n",
      "\n",
      "classifier: LogRegL1\n",
      "|    | feature_extract               |   accuracy |       f1 |   presicion |   recall |\n",
      "|---:|:------------------------------|-----------:|---------:|------------:|---------:|\n",
      "|  0 | CountVect                     |   0.905797 | 0.901515 |    0.908397 | 0.894737 |\n",
      "|  1 | CountVect-2gram               |   0.905797 | 0.901515 |    0.908397 | 0.894737 |\n",
      "|  2 | CountVect-min_df-max_df       |   0.905797 | 0.901515 |    0.908397 | 0.894737 |\n",
      "|  3 | CountVect-2gram-min_df-max_df |   0.905797 | 0.901515 |    0.908397 | 0.894737 |\n",
      "|  4 | TFIDF                         |   0.887681 | 0.883895 |    0.880597 | 0.887218 |\n",
      "|  5 | TFIDF-2gram                   |   0.887681 | 0.883895 |    0.880597 | 0.887218 |\n",
      "|  6 | TFIDF-min_df-max_df           |   0.887681 | 0.883895 |    0.880597 | 0.887218 |\n",
      "|  7 | TFIDF-2gram-min_df-max_df     |   0.887681 | 0.883895 |    0.880597 | 0.887218 |\n",
      "\n",
      "classifier: RandomForest\n",
      "|    | feature_extract               |   accuracy |       f1 |   presicion |   recall |\n",
      "|---:|:------------------------------|-----------:|---------:|------------:|---------:|\n",
      "|  0 | CountVect-2gram-min_df-max_df |   0.913043 | 0.912409 |    0.886525 | 0.93985  |\n",
      "|  1 | TFIDF-2gram                   |   0.913043 | 0.913669 |    0.875862 | 0.954887 |\n",
      "|  2 | TFIDF                         |   0.90942  | 0.908425 |    0.885714 | 0.932331 |\n",
      "|  3 | CountVect                     |   0.902174 | 0.901818 |    0.873239 | 0.932331 |\n",
      "|  4 | CountVect-2gram               |   0.902174 | 0.902527 |    0.868056 | 0.93985  |\n",
      "|  5 | TFIDF-2gram-min_df-max_df     |   0.902174 | 0.901818 |    0.873239 | 0.932331 |\n",
      "|  6 | CountVect-min_df-max_df       |   0.898551 | 0.89781  |    0.87234  | 0.924812 |\n",
      "|  7 | TFIDF-min_df-max_df           |   0.898551 | 0.898551 |    0.867133 | 0.932331 |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parse_result(result, clf_result=clf_result, data=data_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: Different test data graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = make_vocab(data_split)\n",
    "feature_extract = {\n",
    "    'TFIDF': {\n",
    "        'classifier_type': 'TFIDF',\n",
    "        'method': TfidfVectorizer,\n",
    "        'params': {\n",
    "            'tokenizer': word_tokenize,\n",
    "            'stop_words': 'english',\n",
    "            'vocabulary': vocab,\n",
    "            'ngram_range':(1,2),\n",
    "            'min_df': 0.05,\n",
    "            'max_df': 0.95\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "classifiers = {\n",
    "    'LinearSVC': {\n",
    "        'classifier_type': 'Linear SVC',\n",
    "        'method': LinearSVC,\n",
    "        'params':{\n",
    "            'class_weight':'balanced'\n",
    "        }\n",
    "    },\n",
    "    'LogRegL1':{\n",
    "        'classifier_type': 'Logistic Regression L1',\n",
    "        'method': LogisticRegression,\n",
    "        'params':{\n",
    "            'penalty': 'l1',\n",
    "            'class_weight':'balanced',\n",
    "            'solver': 'liblinear',\n",
    "            'max_iter': 1000\n",
    "        }\n",
    "    },\n",
    "    'LogRegL2':{\n",
    "        'classifier_type': 'Logistic Regression L2',\n",
    "        'method': LogisticRegression,\n",
    "        'params':{\n",
    "            'penalty': 'l2',\n",
    "            'class_weight':'balanced',\n",
    "            'solver': 'liblinear',\n",
    "            'max_iter': 1000\n",
    "        }\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'classifier_type':'Random Forest ',\n",
    "        'method': RandomForestClassifier,\n",
    "        'params':{\n",
    "            'class_weight':'balanced'\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "grid_parameters = {\n",
    "    'feature_extract': list(feature_extract.keys()),\n",
    "    'classifier': list(classifiers.keys()),\n",
    "}\n",
    "\n",
    "grid = ParameterGrid(grid_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature = make_method(feature_extract, 'TFIDF')\n",
    "# all_X = df_data['text']\n",
    "# all_Y = df_data['is_flood']\n",
    "# vect = feature.fit(all_X)\n",
    "# x, y = vect.transform(list(all_X)), [1 if t else 0 for t in all_Y]\n",
    "# clf = make_method(classifiers, 'LinearSVC')\n",
    "# cross_validate(clf, x, y, cv=5,\n",
    "#               scoring=('accuracy', 'precision', 'recall', 'f1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded\n",
      "10 270\n",
      "OVERRIDE\n",
      "Feature: TFIDF   Clasifier: LinearSVC   Key: TFIDF-LinearSVC\n",
      "Feature: TFIDF   Clasifier: LogRegL1   Key: TFIDF-LogRegL1\n",
      "Feature: TFIDF   Clasifier: LogRegL2   Key: TFIDF-LogRegL2\n",
      "Feature: TFIDF   Clasifier: RandomForest   Key: TFIDF-RandomForest\n",
      "Data Loaded\n",
      "20 270\n",
      "OVERRIDE\n",
      "Feature: TFIDF   Clasifier: LinearSVC   Key: TFIDF-LinearSVC\n",
      "Feature: TFIDF   Clasifier: LogRegL1   Key: TFIDF-LogRegL1\n",
      "Feature: TFIDF   Clasifier: LogRegL2   Key: TFIDF-LogRegL2\n",
      "Feature: TFIDF   Clasifier: RandomForest   Key: TFIDF-RandomForest\n",
      "Data Loaded\n",
      "50 270\n",
      "OVERRIDE\n",
      "Feature: TFIDF   Clasifier: LinearSVC   Key: TFIDF-LinearSVC\n",
      "Feature: TFIDF   Clasifier: LogRegL1   Key: TFIDF-LogRegL1\n",
      "Feature: TFIDF   Clasifier: LogRegL2   Key: TFIDF-LogRegL2\n",
      "Feature: TFIDF   Clasifier: RandomForest   Key: TFIDF-RandomForest\n",
      "Data Loaded\n",
      "100 270\n",
      "OVERRIDE\n",
      "Feature: TFIDF   Clasifier: LinearSVC   Key: TFIDF-LinearSVC\n",
      "Feature: TFIDF   Clasifier: LogRegL1   Key: TFIDF-LogRegL1\n",
      "Feature: TFIDF   Clasifier: LogRegL2   Key: TFIDF-LogRegL2\n",
      "Feature: TFIDF   Clasifier: RandomForest   Key: TFIDF-RandomForest\n",
      "Data Loaded\n",
      "200 270\n",
      "OVERRIDE\n",
      "Feature: TFIDF   Clasifier: LinearSVC   Key: TFIDF-LinearSVC\n",
      "Feature: TFIDF   Clasifier: LogRegL1   Key: TFIDF-LogRegL1\n",
      "Feature: TFIDF   Clasifier: LogRegL2   Key: TFIDF-LogRegL2\n",
      "Feature: TFIDF   Clasifier: RandomForest   Key: TFIDF-RandomForest\n",
      "Data Loaded\n",
      "500 270\n",
      "OVERRIDE\n",
      "Feature: TFIDF   Clasifier: LinearSVC   Key: TFIDF-LinearSVC\n",
      "Feature: TFIDF   Clasifier: LogRegL1   Key: TFIDF-LogRegL1\n",
      "Feature: TFIDF   Clasifier: LogRegL2   Key: TFIDF-LogRegL2\n",
      "Feature: TFIDF   Clasifier: RandomForest   Key: TFIDF-RandomForest\n",
      "Data Loaded\n",
      "1000 270\n",
      "OVERRIDE\n",
      "Feature: TFIDF   Clasifier: LinearSVC   Key: TFIDF-LinearSVC\n",
      "Feature: TFIDF   Clasifier: LogRegL1   Key: TFIDF-LogRegL1\n",
      "Feature: TFIDF   Clasifier: LogRegL2   Key: TFIDF-LogRegL2\n",
      "Feature: TFIDF   Clasifier: RandomForest   Key: TFIDF-RandomForest\n"
     ]
    }
   ],
   "source": [
    "overall_result = []\n",
    "for train_size in [10,20,50,100,200,500,1000]:\n",
    "    test_size = 270\n",
    "    result, clf_result = {}, {}\n",
    "    debug=True\n",
    "    override=True\n",
    "    data_split = make_data_ratio(df_data, test_size=test_size, train_size=train_size,\n",
    "                               debug=debug, shuffle_seed=global_shuffle_seed, override=override)\n",
    "    print(len(data_split['train']), len(data_split['test']))\n",
    "    actual = [i if i==True else 0 for i in data_split['test']['is_flood']]\n",
    "    clf_result, result = run_grid(grid, data_split, feature_extract, classifiers, clf_result, result, \n",
    "                              debug=debug, override=override)\n",
    "    for key, val in clf_result.items():\n",
    "#         if key not in overall_result: overall_result[key] = []\n",
    "        predict = val['predict']\n",
    "        clf_acc = accuracy_score(actual, predict)\n",
    "        pre, rec, fsc, sup = precision_recall_fscore_support(actual, predict, average='binary')\n",
    "        d = { 'key':key, 'train_size':train_size, 'test_size':test_size, 'accuracy':clf_acc, 'precision':pre, \n",
    "            'recall':rec, 'f1':fsc,'predict':predict, 'actual': actual\n",
    "        }\n",
    "        overall_result.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>train_size</th>\n",
       "      <th>test_size</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TFIDF-RandomForest</td>\n",
       "      <td>10</td>\n",
       "      <td>270</td>\n",
       "      <td>0.485185</td>\n",
       "      <td>0.483271</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.651629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TFIDF-LogRegL1</td>\n",
       "      <td>10</td>\n",
       "      <td>270</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF-LinearSVC</td>\n",
       "      <td>10</td>\n",
       "      <td>270</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.796748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TFIDF-LogRegL2</td>\n",
       "      <td>10</td>\n",
       "      <td>270</td>\n",
       "      <td>0.829630</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.820313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TFIDF-LogRegL1</td>\n",
       "      <td>20</td>\n",
       "      <td>270</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TFIDF-RandomForest</td>\n",
       "      <td>20</td>\n",
       "      <td>270</td>\n",
       "      <td>0.662963</td>\n",
       "      <td>0.591549</td>\n",
       "      <td>0.969231</td>\n",
       "      <td>0.734694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TFIDF-LinearSVC</td>\n",
       "      <td>20</td>\n",
       "      <td>270</td>\n",
       "      <td>0.781481</td>\n",
       "      <td>0.831776</td>\n",
       "      <td>0.684615</td>\n",
       "      <td>0.751055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TFIDF-LogRegL2</td>\n",
       "      <td>20</td>\n",
       "      <td>270</td>\n",
       "      <td>0.796296</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.775510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TFIDF-LogRegL1</td>\n",
       "      <td>50</td>\n",
       "      <td>270</td>\n",
       "      <td>0.662963</td>\n",
       "      <td>0.634483</td>\n",
       "      <td>0.707692</td>\n",
       "      <td>0.669091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TFIDF-RandomForest</td>\n",
       "      <td>50</td>\n",
       "      <td>270</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.762821</td>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.832168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TFIDF-LogRegL2</td>\n",
       "      <td>50</td>\n",
       "      <td>270</td>\n",
       "      <td>0.862963</td>\n",
       "      <td>0.854962</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.858238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TFIDF-LinearSVC</td>\n",
       "      <td>50</td>\n",
       "      <td>270</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.873016</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.859375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TFIDF-LogRegL1</td>\n",
       "      <td>100</td>\n",
       "      <td>270</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.584615</td>\n",
       "      <td>0.684685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TFIDF-LogRegL2</td>\n",
       "      <td>100</td>\n",
       "      <td>270</td>\n",
       "      <td>0.870370</td>\n",
       "      <td>0.886179</td>\n",
       "      <td>0.838462</td>\n",
       "      <td>0.861660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>TFIDF-RandomForest</td>\n",
       "      <td>100</td>\n",
       "      <td>270</td>\n",
       "      <td>0.870370</td>\n",
       "      <td>0.886179</td>\n",
       "      <td>0.838462</td>\n",
       "      <td>0.861660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TFIDF-LinearSVC</td>\n",
       "      <td>100</td>\n",
       "      <td>270</td>\n",
       "      <td>0.877778</td>\n",
       "      <td>0.894309</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.869565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>TFIDF-LogRegL1</td>\n",
       "      <td>200</td>\n",
       "      <td>270</td>\n",
       "      <td>0.803704</td>\n",
       "      <td>0.840708</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.781893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TFIDF-LogRegL2</td>\n",
       "      <td>200</td>\n",
       "      <td>270</td>\n",
       "      <td>0.892593</td>\n",
       "      <td>0.874074</td>\n",
       "      <td>0.907692</td>\n",
       "      <td>0.890566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>TFIDF-RandomForest</td>\n",
       "      <td>200</td>\n",
       "      <td>270</td>\n",
       "      <td>0.892593</td>\n",
       "      <td>0.863309</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.892193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>TFIDF-LinearSVC</td>\n",
       "      <td>200</td>\n",
       "      <td>270</td>\n",
       "      <td>0.896296</td>\n",
       "      <td>0.886364</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.893130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>TFIDF-LogRegL1</td>\n",
       "      <td>500</td>\n",
       "      <td>270</td>\n",
       "      <td>0.870370</td>\n",
       "      <td>0.874016</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.863813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>TFIDF-LogRegL2</td>\n",
       "      <td>500</td>\n",
       "      <td>270</td>\n",
       "      <td>0.903704</td>\n",
       "      <td>0.888060</td>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.901515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>TFIDF-LinearSVC</td>\n",
       "      <td>500</td>\n",
       "      <td>270</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>TFIDF-RandomForest</td>\n",
       "      <td>500</td>\n",
       "      <td>270</td>\n",
       "      <td>0.914815</td>\n",
       "      <td>0.868966</td>\n",
       "      <td>0.969231</td>\n",
       "      <td>0.916364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>TFIDF-LogRegL1</td>\n",
       "      <td>1000</td>\n",
       "      <td>270</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.893130</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.896552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>TFIDF-RandomForest</td>\n",
       "      <td>1000</td>\n",
       "      <td>270</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.898551</td>\n",
       "      <td>0.953846</td>\n",
       "      <td>0.925373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>TFIDF-LogRegL2</td>\n",
       "      <td>1000</td>\n",
       "      <td>270</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.917910</td>\n",
       "      <td>0.946154</td>\n",
       "      <td>0.931818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>TFIDF-LinearSVC</td>\n",
       "      <td>1000</td>\n",
       "      <td>270</td>\n",
       "      <td>0.937037</td>\n",
       "      <td>0.931298</td>\n",
       "      <td>0.938462</td>\n",
       "      <td>0.934866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   key  train_size  test_size  accuracy  precision    recall  \\\n",
       "3   TFIDF-RandomForest          10        270  0.485185   0.483271  1.000000   \n",
       "1       TFIDF-LogRegL1          10        270  0.518519   0.000000  0.000000   \n",
       "0      TFIDF-LinearSVC          10        270  0.814815   0.844828  0.753846   \n",
       "2       TFIDF-LogRegL2          10        270  0.829630   0.833333  0.807692   \n",
       "5       TFIDF-LogRegL1          20        270  0.518519   0.000000  0.000000   \n",
       "7   TFIDF-RandomForest          20        270  0.662963   0.591549  0.969231   \n",
       "4      TFIDF-LinearSVC          20        270  0.781481   0.831776  0.684615   \n",
       "6       TFIDF-LogRegL2          20        270  0.796296   0.826087  0.730769   \n",
       "9       TFIDF-LogRegL1          50        270  0.662963   0.634483  0.707692   \n",
       "11  TFIDF-RandomForest          50        270  0.822222   0.762821  0.915385   \n",
       "10      TFIDF-LogRegL2          50        270  0.862963   0.854962  0.861538   \n",
       "8      TFIDF-LinearSVC          50        270  0.866667   0.873016  0.846154   \n",
       "13      TFIDF-LogRegL1         100        270  0.740741   0.826087  0.584615   \n",
       "14      TFIDF-LogRegL2         100        270  0.870370   0.886179  0.838462   \n",
       "15  TFIDF-RandomForest         100        270  0.870370   0.886179  0.838462   \n",
       "12     TFIDF-LinearSVC         100        270  0.877778   0.894309  0.846154   \n",
       "17      TFIDF-LogRegL1         200        270  0.803704   0.840708  0.730769   \n",
       "18      TFIDF-LogRegL2         200        270  0.892593   0.874074  0.907692   \n",
       "19  TFIDF-RandomForest         200        270  0.892593   0.863309  0.923077   \n",
       "16     TFIDF-LinearSVC         200        270  0.896296   0.886364  0.900000   \n",
       "21      TFIDF-LogRegL1         500        270  0.870370   0.874016  0.853846   \n",
       "22      TFIDF-LogRegL2         500        270  0.903704   0.888060  0.915385   \n",
       "20     TFIDF-LinearSVC         500        270  0.911111   0.895522  0.923077   \n",
       "23  TFIDF-RandomForest         500        270  0.914815   0.868966  0.969231   \n",
       "25      TFIDF-LogRegL1        1000        270  0.900000   0.893130  0.900000   \n",
       "27  TFIDF-RandomForest        1000        270  0.925926   0.898551  0.953846   \n",
       "26      TFIDF-LogRegL2        1000        270  0.933333   0.917910  0.946154   \n",
       "24     TFIDF-LinearSVC        1000        270  0.937037   0.931298  0.938462   \n",
       "\n",
       "          f1  \n",
       "3   0.651629  \n",
       "1   0.000000  \n",
       "0   0.796748  \n",
       "2   0.820313  \n",
       "5   0.000000  \n",
       "7   0.734694  \n",
       "4   0.751055  \n",
       "6   0.775510  \n",
       "9   0.669091  \n",
       "11  0.832168  \n",
       "10  0.858238  \n",
       "8   0.859375  \n",
       "13  0.684685  \n",
       "14  0.861660  \n",
       "15  0.861660  \n",
       "12  0.869565  \n",
       "17  0.781893  \n",
       "18  0.890566  \n",
       "19  0.892193  \n",
       "16  0.893130  \n",
       "21  0.863813  \n",
       "22  0.901515  \n",
       "20  0.909091  \n",
       "23  0.916364  \n",
       "25  0.896552  \n",
       "27  0.925373  \n",
       "26  0.931818  \n",
       "24  0.934866  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(overall_result) \\\n",
    "                .drop(['predict','actual'], axis=1) \\\n",
    "                .sort_values(['train_size', 'accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>train_size</th>\n",
       "      <th>test_size</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BERT-512</td>\n",
       "      <td>10</td>\n",
       "      <td>270</td>\n",
       "      <td>0.748148</td>\n",
       "      <td>0.762712</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.725806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BERT-512</td>\n",
       "      <td>20</td>\n",
       "      <td>270</td>\n",
       "      <td>0.737037</td>\n",
       "      <td>0.715328</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.734082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BERT-512</td>\n",
       "      <td>50</td>\n",
       "      <td>270</td>\n",
       "      <td>0.903704</td>\n",
       "      <td>0.866197</td>\n",
       "      <td>0.946154</td>\n",
       "      <td>0.904412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BERT-512</td>\n",
       "      <td>100</td>\n",
       "      <td>270</td>\n",
       "      <td>0.929630</td>\n",
       "      <td>0.923664</td>\n",
       "      <td>0.930769</td>\n",
       "      <td>0.927203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BERT-512</td>\n",
       "      <td>200</td>\n",
       "      <td>270</td>\n",
       "      <td>0.940741</td>\n",
       "      <td>0.919118</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.939850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BERT-512</td>\n",
       "      <td>500</td>\n",
       "      <td>270</td>\n",
       "      <td>0.948148</td>\n",
       "      <td>0.926471</td>\n",
       "      <td>0.969231</td>\n",
       "      <td>0.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BERT-512</td>\n",
       "      <td>1000</td>\n",
       "      <td>270</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.932331</td>\n",
       "      <td>0.953846</td>\n",
       "      <td>0.942966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        key  train_size  test_size  accuracy  precision    recall        f1\n",
       "0  BERT-512          10        270  0.748148   0.762712  0.692308  0.725806\n",
       "1  BERT-512          20        270  0.737037   0.715328  0.753846  0.734082\n",
       "2  BERT-512          50        270  0.903704   0.866197  0.946154  0.904412\n",
       "3  BERT-512         100        270  0.929630   0.923664  0.930769  0.927203\n",
       "4  BERT-512         200        270  0.940741   0.919118  0.961538  0.939850\n",
       "5  BERT-512         500        270  0.948148   0.926471  0.969231  0.947368\n",
       "6  BERT-512        1000        270  0.944444   0.932331  0.953846  0.942966"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summarize the results\n",
    "overall_result_bert = [\n",
    "    {'key': 'BERT-512', 'train_size': 10, 'test_size': 270, 'accuracy': 0.7481481481481481, \n",
    "     'precision': 0.7627118644067796, 'recall': 0.6923076923076923, 'f1': 0.7258064516129032},\n",
    "    {'key': 'BERT-512', 'train_size': 20, 'test_size': 270, 'accuracy': 0.737037037037037, \n",
    "     'precision': 0.7153284671532847, 'recall': 0.7538461538461538, 'f1': 0.7340823970037453},\n",
    "    {'key': 'BERT-512', 'train_size': 50, 'test_size': 270, 'accuracy': 0.9037037037037037, \n",
    "     'precision': 0.8661971830985915, 'recall': 0.9461538461538461, 'f1': 0.9044117647058824},\n",
    "    {'key': 'BERT-512', 'train_size': 100, 'test_size': 270, 'accuracy': 0.9296296296296296, \n",
    "     'precision': 0.9236641221374046, 'recall': 0.9307692307692308, 'f1': 0.9272030651340997},\n",
    "    {'key': 'BERT-512', 'train_size': 200, 'test_size': 270, 'accuracy': 0.9407407407407408, \n",
    "     'precision': 0.9191176470588235, 'recall': 0.9615384615384616, 'f1': 0.9398496240601504},\n",
    "    {'key': 'BERT-512', 'train_size': 500, 'test_size': 270, 'accuracy': 0.9481481481481482, \n",
    "     'precision': 0.9264705882352942, 'recall': 0.9692307692307692, 'f1': 0.9473684210526316},\n",
    "    {'key': 'BERT-512', 'train_size': 1000, 'test_size': 270, 'accuracy': 0.9444444444444444, \n",
    "     'precision': 0.9323308270676691, 'recall': 0.9538461538461539, 'f1': 0.9429657794676807}\n",
    "]\n",
    "\n",
    "r2 = []\n",
    "for i in overall_result_bert:\n",
    "    i['predict'] = []\n",
    "    i['actual'] = []\n",
    "    r2.append(i)\n",
    "overall_result_bert = r2\n",
    "pd.DataFrame.from_dict(overall_result_bert) \\\n",
    "                .drop(['predict','actual'], axis=1) \\\n",
    "                .sort_values(['train_size', 'accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OVERALL CLASSIFIER RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrr}\n",
      "\\toprule\n",
      "               key &  train\\_size &  test\\_size &  accuracy &  precision &  recall &    f1 \\\\\n",
      "\\midrule\n",
      "TFIDF-RandomForest &          10 &        270 &     0.490 &      0.480 &   1.000 & 0.650 \\\\\n",
      "    TFIDF-LogRegL1 &          10 &        270 &     0.520 &      0.000 &   0.000 & 0.000 \\\\\n",
      "          BERT-512 &          10 &        270 &     0.750 &      0.760 &   0.690 & 0.730 \\\\\n",
      "   TFIDF-LinearSVC &          10 &        270 &     0.810 &      0.840 &   0.750 & 0.800 \\\\\n",
      "    TFIDF-LogRegL2 &          10 &        270 &     0.830 &      0.830 &   0.810 & 0.820 \\\\\n",
      "    TFIDF-LogRegL1 &          20 &        270 &     0.520 &      0.000 &   0.000 & 0.000 \\\\\n",
      "TFIDF-RandomForest &          20 &        270 &     0.660 &      0.590 &   0.970 & 0.730 \\\\\n",
      "          BERT-512 &          20 &        270 &     0.740 &      0.720 &   0.750 & 0.730 \\\\\n",
      "   TFIDF-LinearSVC &          20 &        270 &     0.780 &      0.830 &   0.680 & 0.750 \\\\\n",
      "    TFIDF-LogRegL2 &          20 &        270 &     0.800 &      0.830 &   0.730 & 0.780 \\\\\n",
      "    TFIDF-LogRegL1 &          50 &        270 &     0.660 &      0.630 &   0.710 & 0.670 \\\\\n",
      "TFIDF-RandomForest &          50 &        270 &     0.820 &      0.760 &   0.920 & 0.830 \\\\\n",
      "    TFIDF-LogRegL2 &          50 &        270 &     0.860 &      0.850 &   0.860 & 0.860 \\\\\n",
      "   TFIDF-LinearSVC &          50 &        270 &     0.870 &      0.870 &   0.850 & 0.860 \\\\\n",
      "          BERT-512 &          50 &        270 &     0.900 &      0.870 &   0.950 & 0.900 \\\\\n",
      "    TFIDF-LogRegL1 &         100 &        270 &     0.740 &      0.830 &   0.580 & 0.680 \\\\\n",
      "    TFIDF-LogRegL2 &         100 &        270 &     0.870 &      0.890 &   0.840 & 0.860 \\\\\n",
      "TFIDF-RandomForest &         100 &        270 &     0.870 &      0.890 &   0.840 & 0.860 \\\\\n",
      "   TFIDF-LinearSVC &         100 &        270 &     0.880 &      0.890 &   0.850 & 0.870 \\\\\n",
      "          BERT-512 &         100 &        270 &     0.930 &      0.920 &   0.930 & 0.930 \\\\\n",
      "    TFIDF-LogRegL1 &         200 &        270 &     0.800 &      0.840 &   0.730 & 0.780 \\\\\n",
      "    TFIDF-LogRegL2 &         200 &        270 &     0.890 &      0.870 &   0.910 & 0.890 \\\\\n",
      "TFIDF-RandomForest &         200 &        270 &     0.890 &      0.860 &   0.920 & 0.890 \\\\\n",
      "   TFIDF-LinearSVC &         200 &        270 &     0.900 &      0.890 &   0.900 & 0.890 \\\\\n",
      "          BERT-512 &         200 &        270 &     0.940 &      0.920 &   0.960 & 0.940 \\\\\n",
      "    TFIDF-LogRegL1 &         500 &        270 &     0.870 &      0.870 &   0.850 & 0.860 \\\\\n",
      "    TFIDF-LogRegL2 &         500 &        270 &     0.900 &      0.890 &   0.920 & 0.900 \\\\\n",
      "   TFIDF-LinearSVC &         500 &        270 &     0.910 &      0.900 &   0.920 & 0.910 \\\\\n",
      "TFIDF-RandomForest &         500 &        270 &     0.910 &      0.870 &   0.970 & 0.920 \\\\\n",
      "          BERT-512 &         500 &        270 &     0.950 &      0.930 &   0.970 & 0.950 \\\\\n",
      "    TFIDF-LogRegL1 &        1000 &        270 &     0.900 &      0.890 &   0.900 & 0.900 \\\\\n",
      "TFIDF-RandomForest &        1000 &        270 &     0.930 &      0.900 &   0.950 & 0.930 \\\\\n",
      "    TFIDF-LogRegL2 &        1000 &        270 &     0.930 &      0.920 &   0.950 & 0.930 \\\\\n",
      "   TFIDF-LinearSVC &        1000 &        270 &     0.940 &      0.930 &   0.940 & 0.930 \\\\\n",
      "          BERT-512 &        1000 &        270 &     0.940 &      0.930 &   0.950 & 0.940 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fn/q1_rsjk92n3232jc85ymgdpm0000gn/T/ipykernel_32262/3292481065.py:8: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(result_df.reset_index(drop=True).to_latex(index=False, float_format='%.3f'))\n"
     ]
    }
   ],
   "source": [
    "result_df = pd.DataFrame.from_dict(overall_result + overall_result_bert) \\\n",
    "                .drop(['predict','actual'], axis=1) \\\n",
    "                .sort_values(['train_size', 'accuracy'])\n",
    "result_df['accuracy'] = result_df['accuracy'].apply(lambda x:round(x,2))\n",
    "result_df['precision'] = result_df['precision'].apply(lambda x:round(x,2))\n",
    "result_df['recall'] = result_df['recall'].apply(lambda x:round(x,2))\n",
    "result_df['f1'] = result_df['f1'].apply(lambda x:round(x,2))\n",
    "print(result_df.reset_index(drop=True).to_latex(index=False, float_format='%.3f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 3: cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: TFIDF   Clasifier: LinearSVC   Key: TFIDF-LinearSVC\n",
      "Feature: TFIDF   Clasifier: LogRegL1   Key: TFIDF-LogRegL1\n",
      "Feature: TFIDF   Clasifier: LogRegL2   Key: TFIDF-LogRegL2\n",
      "Feature: TFIDF   Clasifier: RandomForest   Key: TFIDF-RandomForest\n"
     ]
    }
   ],
   "source": [
    "overall_result = []\n",
    "clf_result = {}\n",
    "debug=True\n",
    "override=True\n",
    "clf_result = run_grid_cross_validate(grid, df_data, feature_extract, classifiers, clf_result, debug=debug)\n",
    "for key, val in clf_result.items():\n",
    "    d = { 'key':key,\n",
    "         'mean_accuracy': round(np.mean(val['test_accuracy']),2), \n",
    "         'mean_precision': round(np.mean(val['test_precision']),2),\n",
    "         'mean_recall': round(np.mean(val['test_recall']),2), \n",
    "         'mean_f1': round(np.mean(val['test_f1']),2),\n",
    "         'accuracy':val['test_accuracy'], 'precision':val['test_precision'],\n",
    "         'recall':val['test_recall'], 'f1':val['test_f1']\n",
    "    }\n",
    "    overall_result.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "               key &  mean\\_accuracy &  mean\\_precision &  mean\\_recall &  mean\\_f1 \\\\\n",
      "\\midrule\n",
      "    TFIDF-LogRegL1 &          0.900 &           0.890 &        0.890 &    0.890 \\\\\n",
      "TFIDF-RandomForest &          0.910 &           0.870 &        0.970 &    0.910 \\\\\n",
      "    TFIDF-LogRegL2 &          0.920 &           0.890 &        0.940 &    0.920 \\\\\n",
      "   TFIDF-LinearSVC &          0.930 &           0.900 &        0.950 &    0.930 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fn/q1_rsjk92n3232jc85ymgdpm0000gn/T/ipykernel_32262/1187496952.py:3: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  .sort_values(['mean_accuracy']).to_latex(index=False, float_format='%.3f'))\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame.from_dict(overall_result) \\\n",
    "                .drop(['accuracy','precision', 'recall', 'f1'], axis=1) \\\n",
    "                .sort_values(['mean_accuracy']).to_latex(index=False, float_format='%.3f'))\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 4: 500 train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded\n",
      "500 880\n",
      "OVERRIDE\n",
      "Feature: TFIDF   Clasifier: LinearSVC   Key: TFIDF-LinearSVC\n",
      "Feature: TFIDF   Clasifier: LogRegL1   Key: TFIDF-LogRegL1\n",
      "Feature: TFIDF   Clasifier: LogRegL2   Key: TFIDF-LogRegL2\n",
      "Feature: TFIDF   Clasifier: RandomForest   Key: TFIDF-RandomForest\n"
     ]
    }
   ],
   "source": [
    "overall_result = []\n",
    "train_size = 500\n",
    "test_size = len(df_data)-train_size\n",
    "result, clf_result = {}, {}\n",
    "debug=True\n",
    "override=True\n",
    "data_split = make_data_ratio(df_data, test_size=test_size, train_size=train_size,\n",
    "                           debug=debug, shuffle_seed=global_shuffle_seed, override=override)\n",
    "print(len(data_split['train']), len(data_split['test']))\n",
    "actual = [i if i==True else 0 for i in data_split['test']['is_flood']]\n",
    "clf_result, result = run_grid(grid, data_split, feature_extract, classifiers, clf_result, result, \n",
    "                              debug=debug, override=override)\n",
    "for key, val in clf_result.items():\n",
    "#         if key not in overall_result: overall_result[key] = []\n",
    "    predict = val['predict']\n",
    "    clf_acc = accuracy_score(actual, predict)\n",
    "    pre, rec, fsc, sup = precision_recall_fscore_support(actual, predict, average='binary')\n",
    "    d = { 'key':key, 'train_size':train_size, 'test_size':test_size, 'accuracy':clf_acc, 'precision':pre, \n",
    "        'recall':rec, 'f1':fsc,'predict':predict, 'actual': actual\n",
    "    }\n",
    "    overall_result.append(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrr}\n",
      "\\toprule\n",
      "               key &  train\\_size &  test\\_size &  accuracy &  precision &   recall &       f1 \\\\\n",
      "\\midrule\n",
      "    TFIDF-LogRegL1 &         500 &        880 &  0.885227 &   0.904523 & 0.851064 & 0.876979 \\\\\n",
      "    TFIDF-LogRegL2 &         500 &        880 &  0.904545 &   0.893271 & 0.910165 & 0.901639 \\\\\n",
      "TFIDF-RandomForest &         500 &        880 &  0.913636 &   0.883002 & 0.945626 & 0.913242 \\\\\n",
      "   TFIDF-LinearSVC &         500 &        880 &  0.914773 &   0.897260 & 0.929078 & 0.912892 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fn/q1_rsjk92n3232jc85ymgdpm0000gn/T/ipykernel_32262/1303073198.py:3: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  .sort_values(['train_size', 'accuracy']).to_latex(index=False))\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame.from_dict(overall_result) \\\n",
    "                .drop(['predict','actual'], axis=1) \\\n",
    "                .sort_values(['train_size', 'accuracy']).to_latex(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the best approach to predict is: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root_folder = '/Users/jiaying/Desktop/6506 DSPP/Data Assignment #2/data'\n",
    "newspapers = ['bdnews', 'dailySun', 'prothomalo', 'dailyObserver', 'newAge', \n",
    "              'dhakaTribune', 'thedailystar', 'theIndependent', 'theNewNation']\n",
    "newspapers_files = [os.path.join(root_folder, 'all_paper_data', n+'1_data.json') for n in newspapers]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "idSet = set(df_data['doc_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36123\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>news_keywords</th>\n",
       "      <th>description</th>\n",
       "      <th>keywords</th>\n",
       "      <th>datePublished</th>\n",
       "      <th>dateModified</th>\n",
       "      <th>link</th>\n",
       "      <th>query_info</th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>connect_filename</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Massive blackout brings Bangladesh to its knees</td>\n",
       "      <td>[Bangladesh]</td>\n",
       "      <td>2014-11-01 00:00:00</td>\n",
       "      <td></td>\n",
       "      <td>https://bdnews24.com/bangladesh/2014/11/01/mas...</td>\n",
       "      <td>{'query': 'bangladesh \"floods\"', 'paper': 'bdn...</td>\n",
       "      <td>Massive blackout brings Bangladesh to its knees</td>\n",
       "      <td>Date Published:2014-11-01 00:00:00      \\nSeve...</td>\n",
       "      <td>5da996ad-b1a9-490e-9c89-d8a34ecc5741</td>\n",
       "      <td></td>\n",
       "      <td>bdnews</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Nawaz Sharif praises Bangladesh</td>\n",
       "      <td>[Bangladesh]</td>\n",
       "      <td>2014-11-26 00:00:00</td>\n",
       "      <td></td>\n",
       "      <td>https://bdnews24.com/bangladesh/2014/11/26/naw...</td>\n",
       "      <td>{'query': 'bangladesh \"floods\"', 'paper': 'bdn...</td>\n",
       "      <td>Nawaz Sharif praises Bangladesh</td>\n",
       "      <td>Date Published:2014-11-26 00:00:00      \\nHe e...</td>\n",
       "      <td>d8c9576a-e4b4-4cfe-af54-dd208f699d75</td>\n",
       "      <td></td>\n",
       "      <td>bdnews</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Govt files Tk 1 billion compensation suit for ...</td>\n",
       "      <td>[Bangladesh]</td>\n",
       "      <td>2014-12-10 00:00:00</td>\n",
       "      <td></td>\n",
       "      <td>https://bdnews24.com/bangladesh/2014/12/10/gov...</td>\n",
       "      <td>{'query': 'bangladesh \"floods\"', 'paper': 'bdn...</td>\n",
       "      <td>Govt files Tk 1 billion compensation suit for ...</td>\n",
       "      <td>Date Published:2014-12-10 00:00:00      \\nThe ...</td>\n",
       "      <td>6b0a37dd-636e-4b84-93c3-e61d90e8524f</td>\n",
       "      <td></td>\n",
       "      <td>bdnews</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NIA looking for 11 suspects, RAB hands over li...</td>\n",
       "      <td>[Bangladesh]</td>\n",
       "      <td>2014-11-18 00:00:00</td>\n",
       "      <td></td>\n",
       "      <td>https://bdnews24.com/bangladesh/2014/11/18/nia...</td>\n",
       "      <td>{'query': 'bangladesh \"floods\"', 'paper': 'bdn...</td>\n",
       "      <td>NIA looking for 11 suspects, RAB hands over li...</td>\n",
       "      <td>Date Published:2014-11-18 00:00:00      \\nRAB,...</td>\n",
       "      <td>f38f604f-bc15-4b17-8a3d-61064a54bb1c</td>\n",
       "      <td></td>\n",
       "      <td>bdnews</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1,019 Bangladesh nationals in UAE prison</td>\n",
       "      <td>[Bangladesh]</td>\n",
       "      <td>2014-10-28 00:00:00</td>\n",
       "      <td></td>\n",
       "      <td>https://bdnews24.com/bangladesh/2014/10/28/101...</td>\n",
       "      <td>{'query': 'bangladesh \"floods\"', 'paper': 'bdn...</td>\n",
       "      <td>1,019 Bangladesh nationals in UAE prison</td>\n",
       "      <td>Date Published:2014-10-28 00:00:00      \\nTher...</td>\n",
       "      <td>ca17539b-2939-404e-bfac-21ff9f4ad1f0</td>\n",
       "      <td></td>\n",
       "      <td>bdnews</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  abstract news_keywords                                        description  \\\n",
       "0                           Massive blackout brings Bangladesh to its knees   \n",
       "1                                           Nawaz Sharif praises Bangladesh   \n",
       "2                         Govt files Tk 1 billion compensation suit for ...   \n",
       "3                         NIA looking for 11 suspects, RAB hands over li...   \n",
       "4                                  1,019 Bangladesh nationals in UAE prison   \n",
       "\n",
       "       keywords        datePublished dateModified  \\\n",
       "0  [Bangladesh]  2014-11-01 00:00:00                \n",
       "1  [Bangladesh]  2014-11-26 00:00:00                \n",
       "2  [Bangladesh]  2014-12-10 00:00:00                \n",
       "3  [Bangladesh]  2014-11-18 00:00:00                \n",
       "4  [Bangladesh]  2014-10-28 00:00:00                \n",
       "\n",
       "                                                link  \\\n",
       "0  https://bdnews24.com/bangladesh/2014/11/01/mas...   \n",
       "1  https://bdnews24.com/bangladesh/2014/11/26/naw...   \n",
       "2  https://bdnews24.com/bangladesh/2014/12/10/gov...   \n",
       "3  https://bdnews24.com/bangladesh/2014/11/18/nia...   \n",
       "4  https://bdnews24.com/bangladesh/2014/10/28/101...   \n",
       "\n",
       "                                          query_info  \\\n",
       "0  {'query': 'bangladesh \"floods\"', 'paper': 'bdn...   \n",
       "1  {'query': 'bangladesh \"floods\"', 'paper': 'bdn...   \n",
       "2  {'query': 'bangladesh \"floods\"', 'paper': 'bdn...   \n",
       "3  {'query': 'bangladesh \"floods\"', 'paper': 'bdn...   \n",
       "4  {'query': 'bangladesh \"floods\"', 'paper': 'bdn...   \n",
       "\n",
       "                                            headline  \\\n",
       "0    Massive blackout brings Bangladesh to its knees   \n",
       "1                    Nawaz Sharif praises Bangladesh   \n",
       "2  Govt files Tk 1 billion compensation suit for ...   \n",
       "3  NIA looking for 11 suspects, RAB hands over li...   \n",
       "4           1,019 Bangladesh nationals in UAE prison   \n",
       "\n",
       "                                                text  \\\n",
       "0  Date Published:2014-11-01 00:00:00      \\nSeve...   \n",
       "1  Date Published:2014-11-26 00:00:00      \\nHe e...   \n",
       "2  Date Published:2014-12-10 00:00:00      \\nThe ...   \n",
       "3  Date Published:2014-11-18 00:00:00      \\nRAB,...   \n",
       "4  Date Published:2014-10-28 00:00:00      \\nTher...   \n",
       "\n",
       "                                     id connect_filename newspaper authors  \n",
       "0  5da996ad-b1a9-490e-9c89-d8a34ecc5741                     bdnews          \n",
       "1  d8c9576a-e4b4-4cfe-af54-dd208f699d75                     bdnews          \n",
       "2  6b0a37dd-636e-4b84-93c3-e61d90e8524f                     bdnews          \n",
       "3  f38f604f-bc15-4b17-8a3d-61064a54bb1c                     bdnews          \n",
       "4  ca17539b-2939-404e-bfac-21ff9f4ad1f0                     bdnews          "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def classify_new_data(newspapers_files):\n",
    "    new_data = pd.DataFrame()\n",
    "    text_set = set()\n",
    "    for i,newspapers_path in enumerate(newspapers_files):\n",
    "        temp_data = json.load(open(newspapers_path))\n",
    "        temp_data2 = []\n",
    "        for t in temp_data:\n",
    "            if t['id'] in idSet: continue\n",
    "            temp_dict = t['meta']\n",
    "            for k,v in t['article'].items(): temp_dict[k]=v\n",
    "            temp_dict['id'] = t['id']\n",
    "            temp_dict['connect_filename'] = t.get('connect_filename',None)\n",
    "            temp_dict['newspaper'] = newspapers[i]\n",
    "            if t['article']['text'] not in text_set: text_set.add(t['article']['text'])\n",
    "            else: continue\n",
    "            temp_data2.append(temp_dict)\n",
    "        temp_df = pd.DataFrame(temp_data2)\n",
    "        new_data = pd.concat([new_data, temp_df])\n",
    "    new_data = new_data.fillna(\"\")\n",
    "    new_data = new_data[new_data['connect_filename']==\"\"]\n",
    "    print(len(new_data))\n",
    "    return new_data\n",
    "\n",
    "new_data = classify_new_data(newspapers_files)\n",
    "new_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_data_train_test(classifier, feature, data_df=None, predictions_folder = 'predictions',\n",
    "                         prev_true_data_df=None, prev_false_data_df=None, save=True):\n",
    "#     if data_df is None and prev_false_data_df is None: raise Exception('No df_data or prev_dalse_data_df')\n",
    "#     if prev_false_data_df is not None: data_df = prev_false_data_df\n",
    "    to_keep_cols = ['datePublished', 'text', 'doc_id', 'connect_filename', 'newspaper', 'is_flood']\n",
    "    data_df['new_text'] = data_df['text'].apply(preprocess)\n",
    "    \n",
    "    test_features = feature.transform(list(data_df['new_text']))\n",
    "    test_pred = classifier.predict(test_features)\n",
    "    \n",
    "    data_df['is_flood'] = [bool(i) for i in test_pred]\n",
    "    data_df['doc_id'] = data_df['id']\n",
    "    data_df = data_df[to_keep_cols]\n",
    "    true_new_data = data_df.loc[data_df['is_flood']]\n",
    "    false_new_data = data_df.loc[~data_df['is_flood']]\n",
    "    print('Total New Data: {}\\tTrue new Data: {}'.format(len(data_df), len(true_new_data)))\n",
    "    \n",
    "    if prev_true_data_df is not None: df_true_new_data = pd.concat([prev_true_data_df, true_new_data])\n",
    "    else: df_true_new_data = true_new_data\n",
    "    js = df_true_new_data.to_json(orient='records')\n",
    "    if save: json.dump(json.loads(js), open(os.path.join(predictions_folder, 'predicted_isflood.json'), 'w'), indent=2)\n",
    "    \n",
    "    if prev_false_data_df is not None: df_false_new_data = pd.concat([prev_false_data_df, false_new_data])\n",
    "    else: df_false_new_data = false_new_data\n",
    "    js = df_false_new_data.to_json(orient='records')\n",
    "    if save: json.dump(json.loads(js), open(os.path.join(predictions_folder, 'predicted_not_isflood.json'), 'w'), indent=2)\n",
    "    return df_true_new_data, false_new_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total New Data: 36123\tTrue new Data: 2140\n"
     ]
    }
   ],
   "source": [
    "key = 'TFIDF-LinearSVC'\n",
    "feature = clf_result[key]['feature']\n",
    "classifier = clf_result[key]['clf']\n",
    "prev_true_data_df, prev_false_data_df = None, None\n",
    "# prev_true_data_df, prev_false_data_df = get_new_predicted_data()\n",
    "save = False\n",
    "df_true_new_data, false_new_data = loop_data_train_test(classifier, feature, new_data, 'predictions',\n",
    "                                                        prev_true_data_df, prev_false_data_df, save=save)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datePublished</th>\n",
       "      <th>text</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>connect_filename</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>is_flood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>2016-09-03 00:00:00</td>\n",
       "      <td>Date Published:2016-09-03 00:00:00      \\nIn r...</td>\n",
       "      <td>a333e636-7afc-46a9-a6e7-a49da3ca3d67</td>\n",
       "      <td></td>\n",
       "      <td>bdnews</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>2016-08-29 00:00:00</td>\n",
       "      <td>Date Published:2016-08-29 00:00:00      \\nInfo...</td>\n",
       "      <td>9b0133da-fc4d-4735-b44b-ddacb19cf3e6</td>\n",
       "      <td></td>\n",
       "      <td>bdnews</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>2017-07-23 00:00:00</td>\n",
       "      <td>Date Published:2017-07-23 00:00:00      \\nThe ...</td>\n",
       "      <td>d8d651c7-7b04-4599-972a-4fc452ba4a9e</td>\n",
       "      <td></td>\n",
       "      <td>bdnews</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>2017-06-18 00:00:00</td>\n",
       "      <td>Date Published:2017-06-18 00:00:00      \\nLaxm...</td>\n",
       "      <td>422230e1-c97d-499e-bf84-2f8014ead923</td>\n",
       "      <td></td>\n",
       "      <td>bdnews</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>2017-09-23 00:00:00</td>\n",
       "      <td>Date Published:2017-09-23 00:00:00      \\nRoad...</td>\n",
       "      <td>74b887c9-efb2-4d74-a6ef-5eaf24cf679e</td>\n",
       "      <td></td>\n",
       "      <td>bdnews</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3697</th>\n",
       "      <td>2016-08-02 00:00:00</td>\n",
       "      <td>Date Published:2016-08-02 00:00:00      \\nThe ...</td>\n",
       "      <td>61d48782-dcc6-4190-8f98-73b19c771056</td>\n",
       "      <td></td>\n",
       "      <td>theIndependent</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3702</th>\n",
       "      <td>2016-08-24 00:00:00</td>\n",
       "      <td>Date Published:2016-08-24 00:00:00      \\nIndi...</td>\n",
       "      <td>f533b0df-e589-4e29-8872-74cfd250355f</td>\n",
       "      <td></td>\n",
       "      <td>theIndependent</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3705</th>\n",
       "      <td>2016-07-27 00:00:00</td>\n",
       "      <td>Date Published:2016-07-27 00:00:00      \\nThe ...</td>\n",
       "      <td>e7f78328-d806-442e-805b-3f92e1f2daab</td>\n",
       "      <td></td>\n",
       "      <td>theIndependent</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3763</th>\n",
       "      <td>2019-04-07 10:06:58</td>\n",
       "      <td>Date Published:2019-04-07 10:06:58      \\nEros...</td>\n",
       "      <td>b0c65117-21ba-44c4-a06e-edbd3a43900c</td>\n",
       "      <td></td>\n",
       "      <td>theIndependent</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>2017-08-31 00:00:00</td>\n",
       "      <td>Date Published:2017-08-31 00:00:00      \\nBSS,...</td>\n",
       "      <td>41a0db1b-d3f2-45b9-b078-22f001fc61f3</td>\n",
       "      <td></td>\n",
       "      <td>theNewNation</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2140 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            datePublished                                               text  \\\n",
       "327   2016-09-03 00:00:00  Date Published:2016-09-03 00:00:00      \\nIn r...   \n",
       "365   2016-08-29 00:00:00  Date Published:2016-08-29 00:00:00      \\nInfo...   \n",
       "395   2017-07-23 00:00:00  Date Published:2017-07-23 00:00:00      \\nThe ...   \n",
       "438   2017-06-18 00:00:00  Date Published:2017-06-18 00:00:00      \\nLaxm...   \n",
       "454   2017-09-23 00:00:00  Date Published:2017-09-23 00:00:00      \\nRoad...   \n",
       "...                   ...                                                ...   \n",
       "3697  2016-08-02 00:00:00  Date Published:2016-08-02 00:00:00      \\nThe ...   \n",
       "3702  2016-08-24 00:00:00  Date Published:2016-08-24 00:00:00      \\nIndi...   \n",
       "3705  2016-07-27 00:00:00  Date Published:2016-07-27 00:00:00      \\nThe ...   \n",
       "3763  2019-04-07 10:06:58  Date Published:2019-04-07 10:06:58      \\nEros...   \n",
       "297   2017-08-31 00:00:00  Date Published:2017-08-31 00:00:00      \\nBSS,...   \n",
       "\n",
       "                                    doc_id connect_filename       newspaper  \\\n",
       "327   a333e636-7afc-46a9-a6e7-a49da3ca3d67                           bdnews   \n",
       "365   9b0133da-fc4d-4735-b44b-ddacb19cf3e6                           bdnews   \n",
       "395   d8d651c7-7b04-4599-972a-4fc452ba4a9e                           bdnews   \n",
       "438   422230e1-c97d-499e-bf84-2f8014ead923                           bdnews   \n",
       "454   74b887c9-efb2-4d74-a6ef-5eaf24cf679e                           bdnews   \n",
       "...                                    ...              ...             ...   \n",
       "3697  61d48782-dcc6-4190-8f98-73b19c771056                   theIndependent   \n",
       "3702  f533b0df-e589-4e29-8872-74cfd250355f                   theIndependent   \n",
       "3705  e7f78328-d806-442e-805b-3f92e1f2daab                   theIndependent   \n",
       "3763  b0c65117-21ba-44c4-a06e-edbd3a43900c                   theIndependent   \n",
       "297   41a0db1b-d3f2-45b9-b078-22f001fc61f3                     theNewNation   \n",
       "\n",
       "      is_flood  \n",
       "327       True  \n",
       "365       True  \n",
       "395       True  \n",
       "438       True  \n",
       "454       True  \n",
       "...        ...  \n",
       "3697      True  \n",
       "3702      True  \n",
       "3705      True  \n",
       "3763      True  \n",
       "297       True  \n",
       "\n",
       "[2140 rows x 6 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_true_new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
